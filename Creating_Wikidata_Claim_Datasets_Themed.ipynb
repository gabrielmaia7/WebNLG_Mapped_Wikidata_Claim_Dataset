{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import ast\n",
    "import wikidata_utils as wdutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = '../wikidata_claims_refs_parsed.db'\n",
    "claims_columns = ['entity_id','claim_id','rank','property_id','datatype','datavalue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wd_API = wdutils.CachedWikidataAPI(save_every_x_queries=1000000)\n",
    "Wd_API.languages = ['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/final_entity_list.p','rb') as f:\n",
    "    final_entity_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_entity_labels = {\n",
    "    'Q1248784':'Airport',\n",
    "    'Q11631':'Astronaut',\n",
    "    'Q41176':'Building',\n",
    "    'Q515':'City',\n",
    "    'Q1114461':'ComicsCharacter',\n",
    "    'Q2095':'Food',\n",
    "    'Q4989906':'Monument',\n",
    "    'Q12973014':'SportsTeam',\n",
    "    'Q3918':'University',\n",
    "    'Q47461344':'WrittenWork',\n",
    "    'Q2066131':'Athlete',\n",
    "    'Q483501':'Artist',\n",
    "    'Q6999':'CelestialBody',\n",
    "    'Q334166':'MeanOfTransportation',\n",
    "    'Q82955':'Politician',\n",
    "    'Q16521':'Taxon',\n",
    "    'Q79007':'Street',\n",
    "    'Q3305213':'Painting',\n",
    "    'Q11173':'ChemicalCompound',\n",
    "    'Q8502':'Mountain'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a dataframe of suitable claims for each theme\n",
    "\n",
    "Suitable claims would be:\n",
    "1. Not deprecated\n",
    "2. Not link to external IDs\n",
    "3. Not link to URLs\n",
    "4. Not including P31 and P279 predicated or other predicates later noticed to be not suited for verbalisation.\n",
    "5. No cases with 'novalue' or 'somevalue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with points 1 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing the 67 properties deemed as ontological or unverbalisable WOULD leave us with 91.29 % of the original claims accross all Wikidata.\n",
      "However the removal is not done here and not for all Wikidata. It is best to measure the percentage of claim coverage after all removals took place, per theme, which is the next cell\n"
     ]
    }
   ],
   "source": [
    "properties_to_remove = {\n",
    "    'general':[\n",
    "        'P31', # - instance of\n",
    "        'P279',# - subclass of\n",
    "        'P373',# - commons category\n",
    "        'P910',# - Topic's main category\n",
    "        'P7561',# - category for the interior of the item\n",
    "        'P5008',# - on focus list of Wikimedia project\n",
    "        'P2670',# -  has parts of the class\n",
    "        'P1740',# -  category for films shot at this location\n",
    "        'P1612',# -  Commons Institution page\n",
    "        'P8989',# -  category for the view of the item\n",
    "        'P2959',# -  permanent duplicated item\n",
    "        'P7867',# -  category for maps\n",
    "        'P935' ,# -  Commons gallery\n",
    "        'P1472',#  -  Commons Creator page\n",
    "        'P8596',# category for the exterior of the item\n",
    "        'P5105',# Deutsche Bahn station category\n",
    "        'P8933',# category for the view from the item\n",
    "        'P642',# of\n",
    "        'P3876',# category for alumni of educational institution\n",
    "        'P1791',# category of people buried here\n",
    "        'P7084',# related category\n",
    "        'P1465',# category for people who died here\n",
    "        'P1687',# Wikidata property\n",
    "        'P6104',# maintained by WikiProject\n",
    "        'P4195',# category for employees of the organization\n",
    "        'P1792',# category of associated people\n",
    "        'P5869',# model item\n",
    "        'P1659',# see also\n",
    "        'P1464',# category for people born here\n",
    "        'P2354',# has list\n",
    "        'P1424',# topic's main template\n",
    "        'P7782',# category for ship name\n",
    "        'P179',# part of the series\n",
    "        'P7888',# merged into\n",
    "        'P6365',# member category\n",
    "        'P8464',# content partnership category\n",
    "        'P360',# is a list of\n",
    "        'P805',# statement is subject of\n",
    "        'P8703',# entry in abbreviations table\n",
    "        'P1456',# list of monuments\n",
    "        'P1012',# including\n",
    "        'P1151',# topic's main Wikimedia portal\n",
    "        'P2490',# page at OSTIS Belarus Wiki\n",
    "        'P593',# HomoloGene ID\n",
    "        'P8744',# economy of topic\n",
    "        'P2614',# World Heritage criteria\n",
    "        'P2184',# history of topic\n",
    "        'P9241',# demographics of topic\n",
    "        'P487',#Unicode character\n",
    "        'P1754',#category related to list\n",
    "        'P2559',#Wikidata usage instructions\n",
    "        'P2517',#category for recipients of this award\n",
    "        'P971',#category combines topics\n",
    "        'P6112',# category for members of a team\n",
    "        'P4224',#category contains\n",
    "        'P301',#category's main topic\n",
    "        'P1753',#list related to category\n",
    "        'P1423',#template has topic\n",
    "        'P1204',#Wikimedia portal's main topic\n",
    "        'P3921',#Wikidata SPARQL query equivalent\n",
    "        'P1963',#properties for this type\n",
    "        'P5125',#Wikimedia outline\n",
    "        'P3176',#uses property\n",
    "        'P8952',#inappropriate property for this type\n",
    "        'P2306',#property\n",
    "        'P5193',#Wikidata property example for forms\n",
    "        'P5977',#Wikidata property example for senses\n",
    "    ],\n",
    "    'specific': {}\n",
    "}\n",
    "\n",
    "for theme in root_entity_labels.keys():\n",
    "    properties_to_remove['specific'][theme] = []\n",
    "    \n",
    "## AIRPORT\n",
    "#properties_to_remove['specific']['Q1248784'] = [\n",
    "#    'P585',# -  point in time\n",
    "#    'P1545',#  -  series ordinal\n",
    "#]\n",
    "\n",
    "# AUSTRONAUT\n",
    "properties_to_remove['specific']['Q11631'] = [\n",
    "    #'P8345',#  -  media franchise\n",
    "    #'P2563',#  -  superhuman feature or ability\n",
    "    #'P2546',#  -  sidekick of\n",
    "    #'P144',# -  based on\n",
    "    #'P7047',#  -  enemy of\n",
    "    #'P577',# -  publication date\n",
    "    #'P941',# -  inspired by\n",
    "    #'P4584',#  -  first appearance\n",
    "    #'P5800',#  -  narrative role\n",
    "    #'P1080',#  -  from narrative universe\n",
    "    #'P175',# -  performer\n",
    "    #'P170',# -  creator\n",
    "    #'P767',#  -  contributor to the creative work or subject\n",
    "    'P598',#commander of (DEPRECATED)\n",
    "]\n",
    "\n",
    "if False:\n",
    "    db = sqlite3.connect(DB_PATH)\n",
    "    cursor = db.cursor()\n",
    "    # To see how many out of the total number of stored claims we are excluding by removing the general properties\n",
    "    sql_query = \"select count(*) from claims where property_id in $1;\"\n",
    "    sql_query = sql_query.replace('$1', '(' + ','.join([('\"' + e + '\"') for e in properties_to_remove['general']]) + ')')\n",
    "    cursor.execute(sql_query)\n",
    "    print(cursor.fetchall()) #22343280\n",
    "    # The total is 256557241\n",
    "    db.close()\n",
    "    \n",
    "print('Removing the',len(properties_to_remove['general']),'properties deemed as ontological or unverbalisable',\n",
    "     'WOULD leave us with',round((256557241-22343280)/256557241*100, 2),'% of the original claims accross all Wikidata.')\n",
    "print('However the removal is not done here and not for all Wikidata. It is best to measure the percentage of claim',\n",
    "    'coverage after all removals took place, per theme, which is the next cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Airport\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.27 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 22.64 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 9.6 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.38 %\n",
      "After all removals, we keep 67.11 percent of total claims for theme Q1248784 - Airport \n",
      "\n",
      "Processing Astronaut\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.05 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 24.58 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 6.35 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.07 %\n",
      "After all removals, we keep 68.95 percent of total claims for theme Q11631 - Astronaut \n",
      "\n",
      "Processing Building\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.04 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 29.03 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 15.95 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.01 %\n",
      "After all removals, we keep 54.97 percent of total claims for theme Q41176 - Building \n",
      "\n",
      "Processing City\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.63 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 42.43 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 11.73 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.02 %\n",
      "After all removals, we keep 45.19 percent of total claims for theme Q515 - City \n",
      "\n",
      "Processing ComicsCharacter\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.46 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 16.22 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 20.27 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.31 %\n",
      "After all removals, we keep 62.74 percent of total claims for theme Q1114461 - ComicsCharacter \n",
      "\n",
      "Processing Food\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.04 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 38.92 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 30.19 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.02 %\n",
      "After all removals, we keep 30.84 percent of total claims for theme Q2095 - Food \n",
      "\n",
      "Processing Monument\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.03 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 25.98 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 15.25 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.34 %\n",
      "After all removals, we keep 58.39 percent of total claims for theme Q4989906 - Monument \n",
      "\n",
      "Processing SportsTeam\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.05 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 14.57 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 15.33 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.2 %\n",
      "After all removals, we keep 69.85 percent of total claims for theme Q12973014 - SportsTeam \n",
      "\n",
      "Processing University\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.14 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 54.82 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 12.57 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.01 %\n",
      "After all removals, we keep 32.45 percent of total claims for theme Q3918 - University \n",
      "\n",
      "Processing WrittenWork\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.01 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 10.54 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 7.17 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.01 %\n",
      "After all removals, we keep 82.26 percent of total claims for theme Q47461344 - WrittenWork \n",
      "\n",
      "Processing Athlete\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.28 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 18.31 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 5.98 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.04 %\n",
      "After all removals, we keep 75.38 percent of total claims for theme Q2066131 - Athlete \n",
      "\n",
      "Processing Artist\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.07 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 39.4 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 7.13 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.05 %\n",
      "After all removals, we keep 53.36 percent of total claims for theme Q483501 - Artist \n",
      "\n",
      "Processing CelestialBody\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 6.8 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 6.12 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 5.85 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.0 %\n",
      "After all removals, we keep 81.23 percent of total claims for theme Q6999 - CelestialBody \n",
      "\n",
      "Processing MeanOfTransportation\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.0 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 11.24 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 17.49 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.07 %\n",
      "After all removals, we keep 71.2 percent of total claims for theme Q334166 - MeanOfTransportation \n",
      "\n",
      "Processing Politician\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.05 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 23.02 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 6.97 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.14 %\n",
      "After all removals, we keep 69.82 percent of total claims for theme Q82955 - Politician \n",
      "\n",
      "Processing Taxon\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.01 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 54.18 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 11.73 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.01 %\n",
      "After all removals, we keep 34.08 percent of total claims for theme Q16521 - Taxon \n",
      "\n",
      "Processing Street\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.01 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 18.77 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 16.2 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.0 %\n",
      "After all removals, we keep 65.03 percent of total claims for theme Q79007 - Street \n",
      "\n",
      "Processing Painting\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.03 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 14.34 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 8.31 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.93 %\n",
      "After all removals, we keep 76.4 percent of total claims for theme Q3305213 - Painting \n",
      "\n",
      "Processing ChemicalCompound\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.01 %\n",
      "-    Removing bad datatypes\n",
      "    -    Percentage of bad datatypes: 59.01 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 12.92 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.0 %\n",
      "After all removals, we keep 28.06 percent of total claims for theme Q11173 - ChemicalCompound \n",
      "\n",
      "Processing Mountain\n",
      "-    Removing deprecated\n",
      "    -    Percentage of deprecated: 0.01 %\n",
      "-    Removing bad datatypes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    -    Percentage of bad datatypes: 47.61 %\n",
      "-    Removing bad properties\n",
      "    -    Percentage of ontology (non-domain) properties: 17.54 %\n",
      "-    Removing somevalue/novalue\n",
      "    -    Percentage of somevalue/novalue: 0.2 %\n",
      "After all removals, we keep 34.65 percent of total claims for theme Q8502 - Mountain \n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = sqlite3.connect(DB_PATH)\n",
    "cursor = db.cursor()\n",
    "\n",
    "theme_dfs = {}\n",
    "\n",
    "try:\n",
    "    for theme, theme_label in list(root_entity_labels.items())[:]:\n",
    "\n",
    "        print('Processing',theme_label)\n",
    "\n",
    "        sql_query = \"select * from claims where entity_id in $1;\"\n",
    "        sql_query = sql_query.replace('$1', '(' + ','.join([('\"' + e + '\"') for e in final_entity_list[theme]]) + ')')\n",
    "\n",
    "\n",
    "        cursor.execute(sql_query)\n",
    "        theme_df = pd.DataFrame(cursor.fetchall())\n",
    "        theme_df.columns = claims_columns\n",
    "        \n",
    "        original_theme_df_size = theme_df.shape[0]\n",
    "        last_stage_theme_df_size = original_theme_df_size\n",
    "\n",
    "        print('-    Removing deprecated')\n",
    "\n",
    "        # Remove deprecated\n",
    "        theme_df = theme_df[theme_df['rank'] != 'deprecated'].reset_index(drop=True)\n",
    "        print(\n",
    "            '    -    Percentage of deprecated:',\n",
    "            round((last_stage_theme_df_size-theme_df.shape[0])/original_theme_df_size*100, 2), '%'\n",
    "        )\n",
    "        last_stage_theme_df_size = theme_df.shape[0]\n",
    "\n",
    "        print('-    Removing bad datatypes')\n",
    "\n",
    "        # Remove external_ids, commonsMedia (e.g. photos), globe-coordinates, urls\n",
    "        bad_datatypes = ['commonsMedia','external-id','globe-coordinate','url', 'wikibase-form',\n",
    "                         'geo-shape', 'math', 'musical-notation', 'tabular-data', 'wikibase-sense']\n",
    "        theme_df = theme_df[\n",
    "            theme_df['datatype'].apply(\n",
    "                lambda x : x not in bad_datatypes\n",
    "            )\n",
    "        ].reset_index(drop=True)\n",
    "        print(\n",
    "            '    -    Percentage of bad datatypes:',\n",
    "            round((last_stage_theme_df_size-theme_df.shape[0])/original_theme_df_size*100, 2), '%'\n",
    "        )\n",
    "        last_stage_theme_df_size = theme_df.shape[0]\n",
    "\n",
    "        print('-    Removing bad properties')\n",
    "\n",
    "        # Remove specific properties such as P31 and P279\n",
    "        theme_df = theme_df[\n",
    "            theme_df['property_id'].apply(\n",
    "                lambda x : (x not in properties_to_remove['general']) and (x not in properties_to_remove['specific'][theme])\n",
    "            )\n",
    "        ].reset_index(drop=True)\n",
    "        print(\n",
    "            '    -    Percentage of ontology (non-domain) properties:',\n",
    "            round((last_stage_theme_df_size-theme_df.shape[0])/original_theme_df_size*100, 2), '%'\n",
    "        )\n",
    "        last_stage_theme_df_size = theme_df.shape[0]\n",
    "        \n",
    "        print('-    Removing somevalue/novalue')\n",
    "\n",
    "        # Remove novalue and somevalue\n",
    "        theme_df = theme_df[\n",
    "            theme_df['datavalue'].apply(\n",
    "                lambda x : x not in ['somevalue', 'novalue']\n",
    "            )\n",
    "        ].reset_index(drop=True)\n",
    "        print(\n",
    "            '    -    Percentage of somevalue/novalue:',\n",
    "            round((last_stage_theme_df_size-theme_df.shape[0])/original_theme_df_size*100, 2), '%'\n",
    "        )\n",
    "        last_stage_theme_df_size = theme_df.shape[0]\n",
    "        \n",
    "        print(\n",
    "            'After all removals, we keep',\n",
    "            round(last_stage_theme_df_size/original_theme_df_size*100, 2),\n",
    "            'percent of total claims for theme', theme, '-', theme_label, '\\n'\n",
    "        )\n",
    "        \n",
    "        theme_dfs[theme] = theme_df\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_theme_dfs = {}\n",
    "\n",
    "for theme, theme_df in theme_dfs.items():\n",
    "    \n",
    "    predicate_theme_df = theme_df.groupby('property_id').count()[['entity_id']]\\\n",
    "        .sort_values('entity_id', ascending=False).reset_index()\n",
    "    \n",
    "    predicate_theme_df.columns = ['property_id','frequency_count']\n",
    "    predicate_theme_df['property_label'] = predicate_theme_df['property_id'].apply(Wd_API.get_label)\n",
    "    predicate_theme_df['frequency_percentage'] = predicate_theme_df['frequency_count'].\\\n",
    "        apply(lambda x: x/theme_df.shape[0]*100)\n",
    "\n",
    "    predicate_theme_dfs[theme] = predicate_theme_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Q1248784 - Airport has 56559 claims and 95 predicates.\n",
      "Entity Q11631 - Astronaut has 4064 claims and 92 predicates.\n",
      "Entity Q41176 - Building has 1844597 claims and 503 predicates.\n",
      "Entity Q515 - City has 86123 claims and 199 predicates.\n",
      "Entity Q1114461 - ComicsCharacter has 16017 claims and 158 predicates.\n",
      "Entity Q2095 - Food has 8552 claims and 129 predicates.\n",
      "Entity Q4989906 - Monument has 360537 claims and 287 predicates.\n",
      "Entity Q12973014 - SportsTeam has 68958 claims and 140 predicates.\n",
      "Entity Q3918 - University has 21678 claims and 165 predicates.\n",
      "Entity Q47461344 - WrittenWork has 132467604 claims and 938 predicates.\n",
      "Entity Q2066131 - Athlete has 2881249 claims and 297 predicates.\n",
      "Entity Q483501 - Artist has 2332723 claims and 396 predicates.\n",
      "Entity Q6999 - CelestialBody has 23507578 claims and 171 predicates.\n",
      "Entity Q334166 - MeanOfTransportation has 16324 claims and 110 predicates.\n",
      "Entity Q82955 - Politician has 1661072 claims and 359 predicates.\n",
      "Entity Q16521 - Taxon has 2203672 claims and 189 predicates.\n",
      "Entity Q79007 - Street has 534897 claims and 118 predicates.\n",
      "Entity Q3305213 - Painting has 1045811 claims and 146 predicates.\n",
      "Entity Q11173 - ChemicalCompound has 658435 claims and 190 predicates.\n",
      "Entity Q8502 - Mountain has 225171 claims and 117 predicates.\n"
     ]
    }
   ],
   "source": [
    "for theme in theme_dfs.keys():\n",
    "    theme_dfs[theme].to_csv('./data/theme_dfs/'+ theme + '_claim_theme_df.csv', index=False)\n",
    "    predicate_theme_dfs[theme].to_csv('./data/theme_dfs/'+ theme + '_predicate_theme_df.csv', index=False)\n",
    "    print('Entity',theme,'-',root_entity_labels[theme],'has',\n",
    "          theme_dfs[theme].shape[0],'claims and',predicate_theme_dfs[theme].shape[0],'predicates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sampled dataset\n",
    "\n",
    "**Computation can be initated direcly from here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Q1248784 - Airport has 56559 claims and 95 predicates.\n",
      "Entity Q11631 - Astronaut has 4064 claims and 92 predicates.\n",
      "Entity Q41176 - Building has 1844597 claims and 503 predicates.\n",
      "Entity Q515 - City has 86123 claims and 199 predicates.\n",
      "Entity Q1114461 - ComicsCharacter has 16017 claims and 158 predicates.\n",
      "Entity Q2095 - Food has 8552 claims and 129 predicates.\n",
      "Entity Q4989906 - Monument has 360537 claims and 287 predicates.\n",
      "Entity Q12973014 - SportsTeam has 68958 claims and 140 predicates.\n",
      "Entity Q3918 - University has 21678 claims and 165 predicates.\n",
      "Entity Q47461344 - WrittenWork has 132467604 claims and 938 predicates.\n",
      "Entity Q2066131 - Athlete has 2881249 claims and 297 predicates.\n",
      "Entity Q483501 - Artist has 2332723 claims and 396 predicates.\n",
      "Entity Q6999 - CelestialBody has 23507578 claims and 171 predicates.\n",
      "Entity Q334166 - MeanOfTransportation has 16324 claims and 110 predicates.\n",
      "Entity Q82955 - Politician has 1661072 claims and 359 predicates.\n",
      "Entity Q16521 - Taxon has 2203672 claims and 189 predicates.\n",
      "Entity Q79007 - Street has 534897 claims and 118 predicates.\n",
      "Entity Q3305213 - Painting has 1045811 claims and 146 predicates.\n",
      "Entity Q11173 - ChemicalCompound has 658435 claims and 190 predicates.\n",
      "Entity Q8502 - Mountain has 225171 claims and 117 predicates.\n"
     ]
    }
   ],
   "source": [
    "theme_dfs, predicate_theme_dfs = {}, {}\n",
    "for theme in root_entity_labels.keys():\n",
    "    theme_dfs[theme] = pd.read_csv('./data/theme_dfs/'+ theme + '_claim_theme_df.csv')\n",
    "    predicate_theme_dfs[theme] = pd.read_csv('./data/theme_dfs/'+ theme + '_predicate_theme_df.csv')\n",
    "    print('Entity',theme,'-',root_entity_labels[theme],'has',\n",
    "          theme_dfs[theme].shape[0],'claims and',predicate_theme_dfs[theme].shape[0],'predicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    " \n",
    "# SUPPORTED CONFIDENCE LEVELS: 50%, 68%, 90%, 95%, and 99%\n",
    "confidence_level_constant = [50,.67], [68,.99], [90,1.64], [95,1.96], [99,2.57]\n",
    " \n",
    "# CALCULATE THE SAMPLE SIZE\n",
    "def sample_size(population_size, confidence_level, confidence_interval):\n",
    "    Z = 0.0\n",
    "    p = 0.5\n",
    "    e = confidence_interval/100.0\n",
    "    N = population_size\n",
    "    n_0 = 0.0\n",
    "    n = 0.0\n",
    " \n",
    "    # LOOP THROUGH SUPPORTED CONFIDENCE LEVELS AND FIND THE NUM STD\n",
    "    # DEVIATIONS FOR THAT CONFIDENCE LEVEL\n",
    "    for i in confidence_level_constant:\n",
    "        if i[0] == confidence_level:\n",
    "            Z = i[1]\n",
    "\n",
    "    if Z == 0.0:\n",
    "        return -1\n",
    "\n",
    "    # CALC SAMPLE SIZE\n",
    "    n_0 = ((Z**2) * p * (1-p)) / (e**2)\n",
    "\n",
    "    # ADJUST SAMPLE SIZE FOR FINITE POPULATION\n",
    "    n = n_0 / (1 + ((n_0 - 1) / float(N)) )\n",
    "\n",
    "    return int(math.ceil(n)) # THE SAMPLE SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing theme Q1248784 - Airport\n",
      "After filtering, there remain 0.9934 % of original claims.\n",
      "After filtering, we are looking at 27 filtered predicates.\n",
      "After filtering, we estimate there are 280920 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 382 samples.\n",
      "That is at least 14 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q11631 - Astronaut\n",
      "After filtering, there remain 0.9889 % of original claims.\n",
      "After filtering, we are looking at 57 filtered predicates.\n",
      "After filtering, we estimate there are 20095 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 351 samples.\n",
      "That is at least 6 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q41176 - Building\n",
      "After filtering, there remain 0.9832 % of original claims.\n",
      "After filtering, we are looking at 67 filtered predicates.\n",
      "After filtering, we estimate there are 9068390 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 5 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q515 - City\n",
      "After filtering, there remain 0.9889 % of original claims.\n",
      "After filtering, we are looking at 72 filtered predicates.\n",
      "After filtering, we estimate there are 425840 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 383 samples.\n",
      "That is at least 5 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q1114461 - ComicsCharacter\n",
      "After filtering, there remain 0.9876 % of original claims.\n",
      "After filtering, we are looking at 79 filtered predicates.\n",
      "After filtering, we estimate there are 79095 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 376 samples.\n",
      "That is at least 4 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q2095 - Food\n",
      "After filtering, there remain 0.987 % of original claims.\n",
      "After filtering, we are looking at 64 filtered predicates.\n",
      "After filtering, we estimate there are 42205 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 368 samples.\n",
      "That is at least 5 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q4989906 - Monument\n",
      "After filtering, there remain 0.99 % of original claims.\n",
      "After filtering, we are looking at 62 filtered predicates.\n",
      "After filtering, we estimate there are 1784680 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 384 samples.\n",
      "That is at least 6 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q12973014 - SportsTeam\n",
      "After filtering, there remain 0.991 % of original claims.\n",
      "After filtering, we are looking at 49 filtered predicates.\n",
      "After filtering, we estimate there are 341695 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 383 samples.\n",
      "That is at least 7 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q3918 - University\n",
      "After filtering, there remain 0.9856 % of original claims.\n",
      "After filtering, we are looking at 62 filtered predicates.\n",
      "After filtering, we estimate there are 106830 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 378 samples.\n",
      "That is at least 6 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q47461344 - WrittenWork\n",
      "After filtering, there remain 0.9885 % of original claims.\n",
      "After filtering, we are looking at 21 filtered predicates.\n",
      "After filtering, we estimate there are 654729870 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 18 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q2066131 - Athlete\n",
      "After filtering, there remain 0.9905 % of original claims.\n",
      "After filtering, we are looking at 53 filtered predicates.\n",
      "After filtering, we estimate there are 14269545 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 7 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q483501 - Artist\n",
      "After filtering, there remain 0.9884 % of original claims.\n",
      "After filtering, we are looking at 65 filtered predicates.\n",
      "After filtering, we estimate there are 11528415 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 5 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q6999 - CelestialBody\n",
      "After filtering, there remain 0.9944 % of original claims.\n",
      "After filtering, we are looking at 25 filtered predicates.\n",
      "After filtering, we estimate there are 116883905 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 15 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q334166 - MeanOfTransportation\n",
      "After filtering, there remain 0.9921 % of original claims.\n",
      "After filtering, we are looking at 58 filtered predicates.\n",
      "After filtering, we estimate there are 80975 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 376 samples.\n",
      "That is at least 6 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q82955 - Politician\n",
      "After filtering, there remain 0.9876 % of original claims.\n",
      "After filtering, we are looking at 56 filtered predicates.\n",
      "After filtering, we estimate there are 8201980 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 6 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q16521 - Taxon\n",
      "After filtering, there remain 0.9951 % of original claims.\n",
      "After filtering, we are looking at 27 filtered predicates.\n",
      "After filtering, we estimate there are 10964590 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 14 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q79007 - Street\n",
      "After filtering, there remain 0.9976 % of original claims.\n",
      "After filtering, we are looking at 21 filtered predicates.\n",
      "After filtering, we estimate there are 2667975 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 384 samples.\n",
      "That is at least 18 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q3305213 - Painting\n",
      "After filtering, there remain 0.9943 % of original claims.\n",
      "After filtering, we are looking at 29 filtered predicates.\n",
      "After filtering, we estimate there are 5199260 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 385 samples.\n",
      "That is at least 13 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q11173 - ChemicalCompound\n",
      "After filtering, there remain 0.9916 % of original claims.\n",
      "After filtering, we are looking at 33 filtered predicates.\n",
      "After filtering, we estimate there are 3264510 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 384 samples.\n",
      "That is at least 11 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n",
      "--------------------\n",
      "Processing theme Q8502 - Mountain\n",
      "After filtering, there remain 0.994 % of original claims.\n",
      "After filtering, we are looking at 23 filtered predicates.\n",
      "After filtering, we estimate there are 1119050 filtered claims on the whole Wikidata.\n",
      "A good sample size (95% ci, 5% me) would be 384 samples.\n",
      "That is at least 16 samples per filtered predicate.\n",
      "Generating filtered and sampled claim dataframe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "THR = 0.05\n",
    "SEED = 42\n",
    "sampled_dfs = {}\n",
    "\n",
    "for theme in list(root_entity_labels.keys())[:]:\n",
    "    #if theme == 'Q47461344':\n",
    "    #    continue\n",
    "    print('Processing theme',theme,'-',root_entity_labels[theme])\n",
    "    # First get list of predicates showing up only over THR %\n",
    "    predicate_theme_df = predicate_theme_dfs[theme]\n",
    "    usual_predicates = predicate_theme_df[predicate_theme_df['frequency_percentage'] >= THR].property_id.values\n",
    "    # Get claim dataset with only these non-rare predicates\n",
    "    theme_df = theme_dfs[theme]\n",
    "    usual_theme_df = theme_df[theme_df.apply(lambda x: x['property_id'] in usual_predicates, axis=1)]\n",
    "    # Check what percentage of original claims remains.\n",
    "    print(\n",
    "        'After filtering, there remain',\n",
    "        round(usual_theme_df.shape[0] / theme_df.shape[0], 4),\n",
    "        '% of original claims.'\n",
    "    )\n",
    "    assert usual_theme_df.shape[0] / theme_df.shape[0] > 0.98\n",
    "    # Also check how many individual predicates we would have,\n",
    "    # as we would need at least a few examples of each (or to increase THR)\n",
    "    print('After filtering, we are looking at', len(usual_predicates), 'filtered predicates.')\n",
    "    # Print the total amount of claims left and the sample size according to cohram's.\n",
    "    print('After filtering, we estimate there are', usual_theme_df.shape[0]*5, 'filtered claims on the whole Wikidata.')\n",
    "    ss = sample_size(usual_theme_df.shape[0], 95, 5)\n",
    "    print('A good sample size (95% ci, 5% me) would be', ss, 'samples.')\n",
    "    # Check how many samples of each predicate we would have, hoping it's at least 3.\n",
    "    ss_per_predicate = math.floor(ss/len(usual_predicates))\n",
    "    print('That is at least', ss_per_predicate, 'samples per filtered predicate.')\n",
    "    assert ss_per_predicate > 3\n",
    "    \n",
    "    print('Generating filtered and sampled claim dataframe.')\n",
    "    sampled_dfs[theme] = []\n",
    "    remaining_ss = ss\n",
    "    for pred in usual_predicates[::-1]:\n",
    "        usual_theme_df_this_pred = usual_theme_df[usual_theme_df['property_id'] == pred]\n",
    "        actual_ss_this_pred = min(ss_per_predicate, usual_theme_df_this_pred.shape[0])\n",
    "        sampled_df = usual_theme_df_this_pred.sample(actual_ss_this_pred, random_state = SEED)\n",
    "        sampled_df['sampling_weight_vb'] = '|'.join([\n",
    "            str(usual_theme_df_this_pred.shape[0]),\n",
    "            str(actual_ss_this_pred)\n",
    "        ])\n",
    "        sampled_df['sampling_weight'] = usual_theme_df_this_pred.shape[0]/actual_ss_this_pred\n",
    "        sampled_dfs[theme].append(sampled_df)\n",
    "        \n",
    "        remaining_ss = remaining_ss - actual_ss_this_pred\n",
    "        #print(remaining_ss, len(sampled_dfs[theme]), actual_ss_this_pred)\n",
    "        if remaining_ss > 0:\n",
    "            ss_per_predicate = math.ceil(remaining_ss/(len(usual_predicates) - len(sampled_dfs[theme])))\n",
    "        \n",
    "    sampled_dfs[theme] = pd.concat(sampled_dfs[theme])\n",
    "    \n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 20 sampled Dataframes.\n",
      "Size of sample dataframe of theme Q1248784 - Airport : (382, 8)\n",
      "Size of sample dataframe of theme Q11631 - Astronaut : (351, 8)\n",
      "Size of sample dataframe of theme Q41176 - Building : (385, 8)\n",
      "Size of sample dataframe of theme Q515 - City : (383, 8)\n",
      "Size of sample dataframe of theme Q1114461 - ComicsCharacter : (376, 8)\n",
      "Size of sample dataframe of theme Q2095 - Food : (368, 8)\n",
      "Size of sample dataframe of theme Q4989906 - Monument : (384, 8)\n",
      "Size of sample dataframe of theme Q12973014 - SportsTeam : (383, 8)\n",
      "Size of sample dataframe of theme Q3918 - University : (378, 8)\n",
      "Size of sample dataframe of theme Q47461344 - WrittenWork : (385, 8)\n",
      "Size of sample dataframe of theme Q2066131 - Athlete : (385, 8)\n",
      "Size of sample dataframe of theme Q483501 - Artist : (385, 8)\n",
      "Size of sample dataframe of theme Q6999 - CelestialBody : (385, 8)\n",
      "Size of sample dataframe of theme Q334166 - MeanOfTransportation : (376, 8)\n",
      "Size of sample dataframe of theme Q82955 - Politician : (385, 8)\n",
      "Size of sample dataframe of theme Q16521 - Taxon : (385, 8)\n",
      "Size of sample dataframe of theme Q79007 - Street : (384, 8)\n",
      "Size of sample dataframe of theme Q3305213 - Painting : (385, 8)\n",
      "Size of sample dataframe of theme Q11173 - ChemicalCompound : (384, 8)\n",
      "Size of sample dataframe of theme Q8502 - Mountain : (384, 8)\n"
     ]
    }
   ],
   "source": [
    "print('Saving',len(sampled_dfs),'sampled Dataframes.')\n",
    "for theme, sample_df in sampled_dfs.items():\n",
    "    print('Size of sample dataframe of theme',theme,'-',root_entity_labels[theme], ':', str(sample_df.shape))\n",
    "    sample_df.to_csv('./data/sampled_dfs/'+ theme + '_sampled_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing datavalues and labels\n",
    "\n",
    "To ready these samples for verbalisation, we need to do a few things:\n",
    "- Obtain main label for the subject\n",
    "- Obtain main label for the property\n",
    "- Obtain the object depending on datatype\n",
    "    - If object's datatype is wikibase-item, get main label for object\n",
    "- Obtain qualifiers for the claim [IF USING QUALIFIERS]\n",
    "    - Obtain pertinent labels for the qualifier\n",
    "- Anything with NO LABEL: Resample. We are only taking items with english labels.\n",
    "    \n",
    "Due to the various labels that a claim could have, there are a couple of strategies that we can adopt to better test if verbalisation works in Wikidata.\n",
    "1. Use only main labels (en). These would be the labels that appear in the titles of these entities (items/properties). Because the goal here is not to verify facts yet, coverage of surface forms is not yet so important. This would limit us to an analysis that starts from the assumption that we are targeting *Wikidata's main labels, but not aliases*, which is OK for the purpose of the testing of a verbalisation in a new environment, but not sufficient to test it against the whole data of Wikidata.\n",
    "2. Use randomly drawn labels from the aliases. This would ensure we are capturing Wikidata as a whole better, specially because we are not trying to test single specific labels (the main ones) necessarily. However, this will introduce much more variability in the dataset, and we do not know if we have enough samples to support it. Mainly, this could defeat our argument of low variability within theme-and-predicate strata, meaning our stratification might not be a good representation of the actual population at all.\n",
    "\n",
    "Because of this, we have decided to pursue option 1. It would be good to mention these details in the future when writing the paper.\n",
    "\n",
    "We are also deciding against the use of qualifiers for now, as we'll first target the actual claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sampled Dataframes.\n",
      "Size of sample dataframe of theme Q1248784 - Airport : (382, 8)\n",
      "Size of sample dataframe of theme Q11631 - Astronaut : (351, 8)\n",
      "Size of sample dataframe of theme Q41176 - Building : (385, 8)\n",
      "Size of sample dataframe of theme Q515 - City : (383, 8)\n",
      "Size of sample dataframe of theme Q1114461 - ComicsCharacter : (376, 8)\n",
      "Size of sample dataframe of theme Q2095 - Food : (368, 8)\n",
      "Size of sample dataframe of theme Q4989906 - Monument : (384, 8)\n",
      "Size of sample dataframe of theme Q12973014 - SportsTeam : (383, 8)\n",
      "Size of sample dataframe of theme Q3918 - University : (378, 8)\n",
      "Size of sample dataframe of theme Q47461344 - WrittenWork : (385, 8)\n",
      "Size of sample dataframe of theme Q2066131 - Athlete : (385, 8)\n",
      "Size of sample dataframe of theme Q483501 - Artist : (385, 8)\n",
      "Size of sample dataframe of theme Q6999 - CelestialBody : (385, 8)\n",
      "Size of sample dataframe of theme Q334166 - MeanOfTransportation : (376, 8)\n",
      "Size of sample dataframe of theme Q82955 - Politician : (385, 8)\n",
      "Size of sample dataframe of theme Q16521 - Taxon : (385, 8)\n",
      "Size of sample dataframe of theme Q79007 - Street : (384, 8)\n",
      "Size of sample dataframe of theme Q3305213 - Painting : (385, 8)\n",
      "Size of sample dataframe of theme Q11173 - ChemicalCompound : (384, 8)\n",
      "Size of sample dataframe of theme Q8502 - Mountain : (384, 8)\n"
     ]
    }
   ],
   "source": [
    "sampled_dfs = {}\n",
    "print('Loading sampled Dataframes.')\n",
    "for theme in root_entity_labels.keys():\n",
    "    sampled_dfs[theme] = pd.read_csv('./data/sampled_dfs/'+ theme + '_sampled_df.csv')\n",
    "    print('Size of sample dataframe of theme',theme,'-',root_entity_labels[theme], ':', str(sampled_dfs[theme].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring entity and property labels of Q1248784 - Airport\n",
      "Acquiring entity and property labels of Q11631 - Astronaut\n",
      "Acquiring entity and property labels of Q41176 - Building\n",
      "Acquiring entity and property labels of Q515 - City\n",
      "Acquiring entity and property labels of Q1114461 - ComicsCharacter\n",
      "Acquiring entity and property labels of Q2095 - Food\n",
      "Acquiring entity and property labels of Q4989906 - Monument\n",
      "Acquiring entity and property labels of Q12973014 - SportsTeam\n",
      "Acquiring entity and property labels of Q3918 - University\n",
      "Acquiring entity and property labels of Q47461344 - WrittenWork\n",
      "Acquiring entity and property labels of Q2066131 - Athlete\n",
      "Acquiring entity and property labels of Q483501 - Artist\n",
      "Acquiring entity and property labels of Q6999 - CelestialBody\n",
      "Acquiring entity and property labels of Q334166 - MeanOfTransportation\n",
      "Acquiring entity and property labels of Q82955 - Politician\n",
      "Acquiring entity and property labels of Q16521 - Taxon\n",
      "Acquiring entity and property labels of Q79007 - Street\n",
      "Acquiring entity and property labels of Q3305213 - Painting\n",
      "Acquiring entity and property labels of Q11173 - ChemicalCompound\n",
      "Acquiring entity and property labels of Q8502 - Mountain\n"
     ]
    }
   ],
   "source": [
    "Wd_API.languages = ['en']\n",
    "Wd_API.save_every_x_queries = 1000000000000000000000\n",
    "for theme in sampled_dfs.keys():\n",
    "    print('Acquiring entity and property labels of',theme,'-',root_entity_labels[theme])\n",
    "    sampled_dfs[theme]['entity_label'] = sampled_dfs[theme]['entity_id'].apply(Wd_API.get_label)\n",
    "    sampled_dfs[theme]['property_label'] = sampled_dfs[theme]['property_id'].apply(Wd_API.get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wd_API.x_queries_passed = Wd_API.save_every_x_queries\n",
    "Wd_API.save_entity_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literal data field for a point in time. Given as a date and time with some precision and boundaries. The time is saved internally in the specified calendar model.\n",
    "\n",
    "- time  explicit value for point in time, represented as a timestamp resembling ISO 8601, e.g. +2013-01-01T00:00:00Z. The year is always signed and padded to have between 4 and 16 digits.\n",
    "- timezone  explicit value as a signed integer. Timezone information as an offset from UTC in minutes.\n",
    "- before  explicit integer value for how many units after the given time it could be. The unit is given by the precision.\n",
    "- after  explicit integer value for how many units before the given time it could be. The unit is given by the precision.\n",
    "- precision  explicit value encoded in a shortint. The numbers have the following meaning: 0 - billion years, 1 - hundred million years, ..., 6 - millennium, 7 - century, 8 - decade, 9 - year, 10 - month, 11 - day, 12 - hour, 13 - minute, 14 - second.\n",
    "- calendarmodel  explicit value given as a URI. It identifies the calendar model of the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1st century'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def turn_to_century_or_millennium(y, mode):\n",
    "    y = str(y)\n",
    "    if mode == 'C':\n",
    "        div = 100\n",
    "        group = int(y.rjust(3, '0')[:-2])\n",
    "        mode_name = 'century'\n",
    "    elif mode == 'M':\n",
    "        div = 1000\n",
    "        group = int(y.rjust(4, '0')[:-3])\n",
    "        mode_name = 'millenium'\n",
    "    else:        \n",
    "        raise ValueError('Use mode = C for century and M for millennium')\n",
    "        \n",
    "    if int(y)%div != 0:\n",
    "        group += 1\n",
    "    group = str(group)\n",
    "\n",
    "    group_suffix = (\n",
    "        'st' if group[-1] == '1' else (\n",
    "            'nd' if group[-1] == '2' else (\n",
    "                'rd' if group[-1] == '3' else 'th'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return ' '.join([group+group_suffix, mode_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pdb\n",
    "from datetime import datetime\n",
    "def get_datavalue_given_datatype(row):\n",
    "    dt = row['datatype']\n",
    "    dv = row['datavalue']\n",
    "    \n",
    "    dt_types = ['wikibase-item', 'monolingualtext', 'quantity', 'time', 'string']\n",
    "    if dt not in dt_types:\n",
    "        print(dt)\n",
    "        raise ValueError\n",
    "    else:\n",
    "        try:\n",
    "            if dt == dt_types[0]:\n",
    "                return Wd_API.get_label(ast.literal_eval(dv)['value']['id']) #get label here\n",
    "            elif dt == dt_types[1]:\n",
    "                return ast.literal_eval(dv)['value']['text']\n",
    "            elif dt == dt_types[2]:\n",
    "                dv = ast.literal_eval(dv)\n",
    "                amount, unit = dv['value']['amount'], dv['value']['unit']\n",
    "                if amount[0] == '+':\n",
    "                    amount = amount[1:]\n",
    "                if str(unit) == '1':\n",
    "                    unit = ''\n",
    "                else:\n",
    "                    unit_entity_id = unit.split('/')[-1]\n",
    "                    unit = Wd_API.get_label(unit_entity_id)#get label here\n",
    "                return ' '.join([amount, unit])\n",
    "            elif dt == dt_types[3]:\n",
    "                dv = ast.literal_eval(dv)\n",
    "                time = dv['value']['time']\n",
    "                timezone = dv['value']['timezone']\n",
    "                precision = dv['value']['precision']\n",
    "                assert dv['value']['after'] == 0 and dv['value']['before'] == 0\n",
    "\n",
    "                sufix = 'BC' if time[0] == '-' else ''\n",
    "                time = time[1:]\n",
    "\n",
    "                if precision == 11: #date\n",
    "                    return (datetime.strptime(time, '%Y-%m-%dT00:00:%SZ').strftime('%d/%m/%Y') + sufix)\n",
    "                elif precision == 10: #month\n",
    "                    try:\n",
    "                        return (datetime.strptime(time, '%Y-%m-00T00:00:%SZ').strftime(\"%B of %Y\") + sufix)\n",
    "                    except ValueError:\n",
    "                        return (datetime.strptime(time, '%Y-%m-%dT00:00:%SZ').strftime(\"%B of %Y\") + sufix)\n",
    "                elif precision == 9: #year\n",
    "                    try:\n",
    "                        return (datetime.strptime(time, '%Y-00-00T00:00:%SZ').strftime('%Y') + sufix)\n",
    "                    except ValueError:\n",
    "                        return (datetime.strptime(time, '%Y-%m-%dT00:00:%SZ').strftime('%Y') + sufix)\n",
    "                elif precision == 8: #decade\n",
    "                    try:\n",
    "                        return (datetime.strptime(time, '%Y-00-00T00:00:%SZ').strftime('%Y')[:-1] +'0s' + sufix)\n",
    "                    except ValueError:\n",
    "                        return (datetime.strptime(time, '%Y-%m-%dT00:00:%SZ').strftime('%Y')[:-1] +'0s' + sufix)\n",
    "                elif precision == 7: #century\n",
    "                    try:\n",
    "                        parsed_time = datetime.strptime(time, '%Y-00-00T00:00:%SZ')\n",
    "                    except ValueError:\n",
    "                        parsed_time = datetime.strptime(time, '%Y-%m-%dT00:00:%SZ')\n",
    "                    finally:                        \n",
    "                        return turn_to_century_or_millennium(\n",
    "                            parsed_time.strftime('%Y'), mode='C'\n",
    "                        ) + sufix\n",
    "                elif precision == 6: #millennium\n",
    "                    try:\n",
    "                        parsed_time = datetime.strptime(time, '%Y-00-00T00:00:%SZ')\n",
    "                    except ValueError:\n",
    "                        parsed_time = datetime.strptime(time, '%Y-%m-%dT00:00:%SZ')\n",
    "                    finally:                        \n",
    "                        return turn_to_century_or_millennium(\n",
    "                            parsed_time.strftime('%Y'), mode='M'\n",
    "                        ) + sufix\n",
    "                elif precision == 4: #hundred thousand years \n",
    "                    timeint = int(datetime.strptime(time, '%Y-00-00T00:00:%SZ').strftime('%Y'))\n",
    "                    timeint = round(timeint/1e5,1)\n",
    "                    return (str(timeint) + 'hundred thousand years' + sufix)\n",
    "                elif precision == 3: #million years \n",
    "                    timeint = int(datetime.strptime(time, '%Y-00-00T00:00:%SZ').strftime('%Y'))\n",
    "                    timeint = round(timeint/1e6,1)\n",
    "                    return (str(timeint) + 'million years' + sufix)\n",
    "                elif precision == 0: #billion years \n",
    "                    timeint = int(datetime.strptime(time, '%Y-00-00T00:00:%SZ').strftime('%Y'))\n",
    "                    timeint = round(timeint/1e9,1)\n",
    "                    return (str(timeint) + 'billion years' +sufix)\n",
    "            elif dt == dt_types[4]:\n",
    "                return ast.literal_eval(dv)['value']\n",
    "        except ValueError as e:\n",
    "            #pdb.set_trace()\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring object labels of Q1248784 - Airport\n",
      "Acquiring object labels of Q11631 - Astronaut\n",
      "Acquiring object labels of Q41176 - Building\n",
      "Acquiring object labels of Q515 - City\n",
      "Acquiring object labels of Q1114461 - ComicsCharacter\n",
      "Acquiring object labels of Q2095 - Food\n",
      "Acquiring object labels of Q4989906 - Monument\n",
      "Acquiring object labels of Q12973014 - SportsTeam\n",
      "Acquiring object labels of Q3918 - University\n",
      "Acquiring object labels of Q47461344 - WrittenWork\n",
      "Acquiring object labels of Q2066131 - Athlete\n",
      "Acquiring object labels of Q483501 - Artist\n",
      "Acquiring object labels of Q6999 - CelestialBody\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wikidata redirect detected.  Input entity id=Q16578582. Returned entity id=Q21500224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring object labels of Q334166 - MeanOfTransportation\n",
      "Acquiring object labels of Q82955 - Politician\n",
      "Acquiring object labels of Q16521 - Taxon\n",
      "Acquiring object labels of Q79007 - Street\n",
      "Acquiring object labels of Q3305213 - Painting\n",
      "Acquiring object labels of Q11173 - ChemicalCompound\n"
     ]
    }
   ],
   "source": [
    "for theme, sampled_df in sampled_dfs.items():\n",
    "    print('Acquiring object labels of',theme,'-',root_entity_labels[theme])\n",
    "    sampled_df['datevalue'] = sampled_df.apply(get_datavalue_given_datatype, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>claim_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>property_id</th>\n",
       "      <th>datatype</th>\n",
       "      <th>datavalue</th>\n",
       "      <th>sampling_weight_vb</th>\n",
       "      <th>sampling_weight</th>\n",
       "      <th>entity_label</th>\n",
       "      <th>property_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Q890949</td>\n",
       "      <td>Q890949$94713524-26dc-4a83-af5d-10947345f251</td>\n",
       "      <td>normal</td>\n",
       "      <td>P3896</td>\n",
       "      <td>geo-shape</td>\n",
       "      <td>{'value': 'Data:Map of Bohumn, Karvin Distri...</td>\n",
       "      <td>86|6</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>Bohumn</td>\n",
       "      <td>geoshape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Q1000033</td>\n",
       "      <td>Q1000033$e8cba304-be90-48ea-854c-d6dc7154a1af</td>\n",
       "      <td>normal</td>\n",
       "      <td>P3896</td>\n",
       "      <td>geo-shape</td>\n",
       "      <td>{'value': 'Data:Map of Lankroun, st nad Orl...</td>\n",
       "      <td>86|6</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>Lankroun</td>\n",
       "      <td>geoshape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Q708616</td>\n",
       "      <td>Q708616$30143A12-9F98-423B-BC12-1C4986F705B8</td>\n",
       "      <td>normal</td>\n",
       "      <td>P3896</td>\n",
       "      <td>geo-shape</td>\n",
       "      <td>{'value': 'Data:India/West Bengal/South 24 Par...</td>\n",
       "      <td>86|6</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>Baruipur</td>\n",
       "      <td>geoshape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Q161513</td>\n",
       "      <td>Q161513$969d84d1-c358-4b70-9c12-27c9dfbcc382</td>\n",
       "      <td>normal</td>\n",
       "      <td>P3896</td>\n",
       "      <td>geo-shape</td>\n",
       "      <td>{'value': 'Data:Map of Tinov, Brno-venkov Dis...</td>\n",
       "      <td>86|6</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>Tinov</td>\n",
       "      <td>geoshape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Q134494</td>\n",
       "      <td>Q134494$663AC6AF-6212-47EA-83FB-00C7277C24DC</td>\n",
       "      <td>normal</td>\n",
       "      <td>P3896</td>\n",
       "      <td>geo-shape</td>\n",
       "      <td>{'value': 'Data:Hungary/Gyr.map', 'type': 'st...</td>\n",
       "      <td>86|6</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>Gyr</td>\n",
       "      <td>geoshape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Q386988</td>\n",
       "      <td>Q386988$5b3caf18-531a-4394-9408-f76ab09ae2bc</td>\n",
       "      <td>normal</td>\n",
       "      <td>P3896</td>\n",
       "      <td>geo-shape</td>\n",
       "      <td>{'value': 'Data:Map of any, Praha-vchod Di...</td>\n",
       "      <td>86|6</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>any</td>\n",
       "      <td>geoshape</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity_id                                       claim_id    rank  \\\n",
       "101   Q890949   Q890949$94713524-26dc-4a83-af5d-10947345f251  normal   \n",
       "102  Q1000033  Q1000033$e8cba304-be90-48ea-854c-d6dc7154a1af  normal   \n",
       "103   Q708616   Q708616$30143A12-9F98-423B-BC12-1C4986F705B8  normal   \n",
       "104   Q161513   Q161513$969d84d1-c358-4b70-9c12-27c9dfbcc382  normal   \n",
       "105   Q134494   Q134494$663AC6AF-6212-47EA-83FB-00C7277C24DC  normal   \n",
       "106   Q386988   Q386988$5b3caf18-531a-4394-9408-f76ab09ae2bc  normal   \n",
       "\n",
       "    property_id   datatype                                          datavalue  \\\n",
       "101       P3896  geo-shape  {'value': 'Data:Map of Bohumn, Karvin Distri...   \n",
       "102       P3896  geo-shape  {'value': 'Data:Map of Lankroun, st nad Orl...   \n",
       "103       P3896  geo-shape  {'value': 'Data:India/West Bengal/South 24 Par...   \n",
       "104       P3896  geo-shape  {'value': 'Data:Map of Tinov, Brno-venkov Dis...   \n",
       "105       P3896  geo-shape  {'value': 'Data:Hungary/Gyr.map', 'type': 'st...   \n",
       "106       P3896  geo-shape  {'value': 'Data:Map of any, Praha-vchod Di...   \n",
       "\n",
       "    sampling_weight_vb  sampling_weight entity_label property_label  \n",
       "101               86|6        14.333333      Bohumn       geoshape  \n",
       "102               86|6        14.333333    Lankroun       geoshape  \n",
       "103               86|6        14.333333     Baruipur       geoshape  \n",
       "104               86|6        14.333333       Tinov       geoshape  \n",
       "105               86|6        14.333333         Gyr       geoshape  \n",
       "106               86|6        14.333333       any       geoshape  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sampled_dfs['Q515'][sampled_dfs['Q515']['datatype'] == 'geo-shape'].copy()\n",
    "#df['datevalue'] = df.apply(get_datavalue_given_datatype, axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
